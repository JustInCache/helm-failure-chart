<p align="center">
  <img src="https://img.shields.io/badge/Helm-v3-blue?logo=helm&logoColor=white" alt="Helm v3"/>
  <img src="https://img.shields.io/badge/Kubernetes-1.24+-326CE5?logo=kubernetes&logoColor=white" alt="Kubernetes"/>
  <img src="https://img.shields.io/badge/License-MIT-green" alt="MIT License"/>
  <img src="https://img.shields.io/github/stars/JustInCache/helm-failure-chart?style=social" alt="GitHub Stars"/>
</p>

<h1 align="center">ğŸ’¥ helm-failure-chart</h1>

<p align="center">
  <strong>A deliberately broken Helm chart with 10+ real-world Kubernetes failure scenarios.</strong><br/>
  Deploy. Break. Diagnose. Fix. Repeat.
</p>

<p align="center">
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-failure-scenarios">Scenarios</a> â€¢
  <a href="#-usage-patterns">Usage</a> â€¢
  <a href="#%EF%B8%8F-customization">Customize</a> â€¢
  <a href="#-contributing">Contribute</a>
</p>

---

## ğŸ¤” What Is This?

Ever wanted a **safe playground** of Kubernetes failures you can deploy on demand? This Helm chart ships with 10+ intentionally broken resources â€” each producing a different, real-world K8s failure.

Perfect for:

- ğŸ¤– **AI Agent Demos** â€” Let your AI bot (n8n + Slack + EKS MCP) diagnose and fix live cluster issues
- ğŸ“ **SRE Training & Interviews** â€” Test troubleshooting skills on realistic failures
- ğŸ“Š **Monitoring Validation** â€” Verify your Prometheus/Grafana/PagerDuty alerts actually fire
- ğŸ§ª **Chaos Engineering** â€” Controlled failure injection without the unpredictability

> ğŸ”’ Everything is **namespace-scoped** and isolated â€” safe to deploy alongside production workloads.

---

## ğŸš€ Quick Start

```bash
# Clone the repo
git clone https://github.com/JustInCache/helm-failure-chart.git
cd helm-failure-chart

# Deploy to any namespace you like
helm install failure-demo . --namespace failure-demo --create-namespace

# ğŸ¿ Sit back and watch the chaos unfold
kubectl get pods -n failure-demo -w
```

---

## ğŸ“¦ What Gets Deployed

| Component | Image | Replicas | Purpose |
|-----------|-------|----------|---------|
| ğŸŒ **Frontend** | `nginx` | 2 | Web UI (static files) |
| âš™ï¸ **Backend** | `node:18-alpine` | 2 | REST API |
| ğŸ”§ **Worker** | `python:3.11-slim` | 1 | Background job processor |
| ğŸ—„ï¸ **Redis** | `redis:7-alpine` | 1 | Cache / message queue |

**Also creates:** ConfigMap, Secret, ServiceAccount, Role, RoleBinding, Ingress, HPA, PVC, NetworkPolicy

> ğŸ’¡ **Resource footprint:** ~850m CPU, ~900Mi memory in requests (most pods fail before consuming anything).

---

## ğŸ”¥ Want Most Common Scenarios ONLY?

If you want the lite chart covers the **5 most common** Kubernetes failures. You can switch to below: 

ğŸ‘‰ **[helm-failure-chart-lite](https://github.com/JustInCache/helm-failure-chart-lite)** â€” the lite version

| | Lite | Full (this repo) |
|---|---|---|
| Scenarios | 5 | 10+ |
| Components | 3 (Frontend, Backend, Worker) | 4 (+ Redis) |
| Ingress / HPA / PVC / NetworkPolicy | âŒ | âœ… |
| Resource footprint | ~200m CPU, ~256Mi | ~850m CPU, ~900Mi |
| Best for | Quick demos, interviews | Comprehensive training, deep dives |

---

## ğŸ’£ Failure Scenarios

### 1ï¸âƒ£ ImagePullBackOff

| | |
|---|---|
| ğŸ“ **Component** | Frontend Deployment |
| ğŸ› **Root Cause** | Image tag `nginx:v99.99.99` does not exist |
| ğŸ‘€ **What You See** | Pod stuck in `ImagePullBackOff` / `ErrImagePull` |
| âœ… **How to Fix** | Change `frontend.image.tag` to a valid tag like `1.25-alpine` |

```bash
kubectl describe pod -l app=frontend -n failure-demo | grep -A5 "Events"
```

---

### 2ï¸âƒ£ CrashLoopBackOff

| | |
|---|---|
| ğŸ“ **Component** | Backend Deployment |
| ğŸ› **Root Cause** | Liveness probe targets port `8080` but container only has port `3000`. Container runs a simple `setTimeout`, not an HTTP server. |
| ğŸ‘€ **What You See** | Pod enters `CrashLoopBackOff` after repeated probe failures |
| âœ… **How to Fix** | Change probe ports to `3000` in `values.yaml`, or remove the HTTP probes |

```bash
kubectl logs -l app=backend -n failure-demo --previous
```

---

### 3ï¸âƒ£ Service Has 0 Endpoints

| | |
|---|---|
| ğŸ“ **Component** | Backend Service |
| ğŸ› **Root Cause** | Service selector is `app: backend-api` but pods are labeled `app: backend` |
| ğŸ‘€ **What You See** | `kubectl get endpoints` shows 0 endpoints â€” traffic never reaches pods |
| âœ… **How to Fix** | Change the service selector to `app: backend` |

```bash
kubectl get endpoints -n failure-demo
```

---

### 4ï¸âƒ£ CreateContainerConfigError

| | |
|---|---|
| ğŸ“ **Component** | Worker Deployment |
| ğŸ› **Root Cause** | Env vars reference ConfigMap keys `DATABASE_HOST` / `DATABASE_PORT`, but the ConfigMap defines `DB_HOST` / `DB_PORT` |
| ğŸ‘€ **What You See** | Pod stuck in `CreateContainerConfigError` |
| âœ… **How to Fix** | Align the key names â€” update the ConfigMap or the deployment env refs |

```bash
kubectl describe pod -l app=worker -n failure-demo | grep -A3 "Warning"
```

---

### 5ï¸âƒ£ OOMKilled

| | |
|---|---|
| ğŸ“ **Component** | Redis Deployment |
| ğŸ› **Root Cause** | Memory limit is `5Mi` â€” Redis needs ~30-50Mi minimum to start |
| ğŸ‘€ **What You See** | Pod gets `OOMKilled` immediately, restarts in a loop |
| âœ… **How to Fix** | Increase `redis.resources.limits.memory` to at least `128Mi` |

```bash
kubectl get pod -l app=redis -n failure-demo -o jsonpath='{.items[0].status.containerStatuses[0].lastState}'
```

---

### 6ï¸âƒ£ Ingress Misconfiguration

| | |
|---|---|
| ğŸ“ **Component** | Ingress |
| ğŸ› **Root Cause** | Frontend path routes to service `xxx-web` (should be `xxx-frontend`). API path uses port `8080` (should be `3000`). |
| ğŸ‘€ **What You See** | 503 errors, no healthy targets |
| âœ… **How to Fix** | Correct the service name and port in `ingress.yaml` |

```bash
kubectl describe ingress -n failure-demo
```

---

### 7ï¸âƒ£ HPA Target Mismatch

| | |
|---|---|
| ğŸ“ **Component** | HorizontalPodAutoscaler |
| ğŸ› **Root Cause** | HPA targets deployment `xxx-backend-api` but actual name is `xxx-backend` |
| ğŸ‘€ **What You See** | HPA shows `<unknown>` for current metrics |
| âœ… **How to Fix** | Fix `scaleTargetRef.name` in `hpa.yaml` to match the actual deployment |

```bash
kubectl get hpa -n failure-demo
```

---

### 8ï¸âƒ£ PVC Stuck in Pending

| | |
|---|---|
| ğŸ“ **Component** | PersistentVolumeClaim |
| ğŸ› **Root Cause** | References StorageClass `gp3-encrypted-premium` which doesn't exist |
| ğŸ‘€ **What You See** | PVC stays `Pending`, worker pod can't mount volume |
| âœ… **How to Fix** | Change `persistence.storageClassName` to an existing class (`gp2`, `gp3`, `standard`) |

```bash
kubectl get pvc -n failure-demo
kubectl get storageclass
```

---

### 9ï¸âƒ£ RBAC Permission Denied

| | |
|---|---|
| ğŸ“ **Component** | Role |
| ğŸ› **Root Cause** | Role lists `deployments` under apiGroup `""` (core) â€” they belong to `"apps"` |
| ğŸ‘€ **What You See** | `403 Forbidden` when the ServiceAccount tries to access deployments |
| âœ… **How to Fix** | Change `apiGroups: [""]` to `apiGroups: ["apps"]` for the deployments rule |

```bash
kubectl auth can-i list deployments \
  --as=system:serviceaccount:failure-demo:failure-demo-helm-failure-chart-sa \
  -n failure-demo
```

---

### ğŸ”Ÿ NetworkPolicy Blocks Database

| | |
|---|---|
| ğŸ“ **Component** | NetworkPolicy |
| ğŸ› **Root Cause** | Backend egress only allows Redis. No rule for PostgreSQL on port 5432. |
| ğŸ‘€ **What You See** | Backend â†’ database connections time out |
| âœ… **How to Fix** | Add an egress rule for the database service on port `5432` |

---

### ğŸ Bonus â€” Secret Key Mismatch

| | |
|---|---|
| ğŸ“ **Component** | Backend Deployment + Secret |
| ğŸ› **Root Cause** | Deployment references `DATABASE_USERNAME` from the secret, but the secret key is `DB_USERNAME` |
| ğŸ‘€ **What You See** | `CreateContainerConfigError` on backend pods |
| âœ… **How to Fix** | Align the key name in either the secret or the deployment |

### ğŸ Bonus â€” Invalid IAM Role ARN

| | |
|---|---|
| ğŸ“ **Component** | ServiceAccount |
| ğŸ› **Root Cause** | IRSA annotation points to `arn:aws:iam::123456789012:role/app-nonexistent-role` |
| ğŸ‘€ **What You See** | AWS API calls from pods fail with auth errors |
| âœ… **How to Fix** | Update the ARN to a valid IAM role, or remove the annotation |

---

## ğŸ¯ Usage Patterns

### ğŸ¤– AI Agent Demo (n8n + Slack + EKS MCP)

1. Deploy the chart to your EKS cluster
2. Ask in Slack: *"What's wrong with pods in the failure-demo namespace?"*
3. AI agent uses EKS MCP to inspect pods, events, services, etc.
4. Agent diagnoses failures and suggests fixes
5. Apply fixes iteratively and re-ask to validate

### ğŸ“ SRE Training / Interviews

Deploy the chart and ask candidates to:
- ğŸ” Identify all failing resources and their root causes
- ğŸ› ï¸ Propose fixes without looking at the source
- ğŸ“‹ Prioritize which failures to fix first

### ğŸ“Š Monitoring & Alerting Validation

Use the chart to verify that your Prometheus/Grafana/PagerDuty pipeline correctly detects:
- `CrashLoopBackOff` and `OOMKilled` pod states
- Deployments with 0 available replicas
- PVCs stuck in Pending
- Services with 0 endpoints

---

## âš™ï¸ Customization

Override any value at install time:

```bash
# Disable ingress (e.g., if you use Kong and don't want ALB conflicts)
helm install failure-demo . -n failure-demo --create-namespace \
  --set ingress.enabled=false

# Disable persistence (skip PVC scenario)
helm install failure-demo . -n failure-demo --create-namespace \
  --set persistence.enabled=false

# Fix the frontend image to isolate other scenarios
helm install failure-demo . -n failure-demo --create-namespace \
  --set frontend.image.tag=1.25-alpine
```

---

## ğŸ›¡ï¸ Safety

Everything is **namespace-scoped**. Nothing touches other namespaces or creates cluster-wide resources.

| Check | Status |
|---|---|
| ğŸ” Cluster-scoped RBAC (ClusterRole) | âœ… None |
| ğŸŒ LoadBalancer / NodePort services | âœ… None â€” all ClusterIP |
| ğŸ”— Cross-namespace NetworkPolicy | âœ… None |
| ğŸ“¦ CRDs / Webhooks | âœ… None |
| ğŸšª Ingress controller side effects | âš ï¸ Only if ALB controller exists â€” disable with `ingress.enabled=false` |

---

## ğŸ§¹ Cleanup

```bash
helm uninstall failure-demo -n failure-demo
kubectl delete namespace failure-demo
```

---

## ğŸ“ Chart Structure

```
helm-failure-chart/
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ values.yaml
â”œâ”€â”€ README.md
â””â”€â”€ templates/
    â”œâ”€â”€ _helpers.tpl
    â”œâ”€â”€ backend-deployment.yaml    # Scenarios 2, Bonus (secret mismatch)
    â”œâ”€â”€ backend-service.yaml       # Scenario 3
    â”œâ”€â”€ configmap.yaml             # Scenario 4 (key mismatch source)
    â”œâ”€â”€ frontend-deployment.yaml   # Scenario 1
    â”œâ”€â”€ frontend-service.yaml
    â”œâ”€â”€ hpa.yaml                   # Scenario 7
    â”œâ”€â”€ ingress.yaml               # Scenario 6
    â”œâ”€â”€ network-policy.yaml        # Scenario 10
    â”œâ”€â”€ pvc.yaml                   # Scenario 8
    â”œâ”€â”€ rbac.yaml                  # Scenario 9
    â”œâ”€â”€ redis-deployment.yaml      # Scenario 5
    â”œâ”€â”€ redis-service.yaml
    â”œâ”€â”€ secret.yaml                # Bonus (key mismatch source)
    â”œâ”€â”€ serviceaccount.yaml        # Bonus (invalid IAM ARN)
    â””â”€â”€ worker-deployment.yaml     # Scenario 4
```

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

- ğŸ› **Add new failure scenarios** â€” open a PR with a new template and update the README
- ğŸ“ **Improve documentation** â€” typos, better explanations, diagrams
- ğŸ§ª **Test on different clusters** â€” EKS, GKE, AKS, minikube, kind â€” report what works
- ğŸ’¡ **Suggest ideas** â€” open an issue with your use case

### How to contribute

1. Fork the repo
2. Create a feature branch (`git checkout -b feat/new-scenario`)
3. Commit your changes (`git commit -m "Add new failure scenario"`)
4. Push to the branch (`git push origin feat/new-scenario`)
5. Open a Pull Request

---

## â­ Star This Repo

If this chart helped you demo, learn, or break things in a fun way â€” **give it a star!** â­

It helps others discover this project and motivates continued development.

[![GitHub stars](https://img.shields.io/github/stars/JustInCache/helm-failure-chart?style=for-the-badge&logo=github&label=Star%20This%20Repo)](https://github.com/JustInCache/helm-failure-chart)

---

## â˜• Buy Me a Coffee

If this project saved you time or sparked an idea, consider buying me a coffee!

<a href="https://buymeacoffee.com/connectankush">
  <img src="assets/bmc.png" alt="Buy Me a Coffee" width="200"/>
</a>

Your support keeps this project maintained and growing ğŸ™

<a href="https://buymeacoffee.com/connectankush">
  <img src="assets/bmc-qr-code.png" alt="Buy Me a Coffee QR Code" width="150"/>
</a>

**[â˜• buymeacoffee.com/connectankush](https://buymeacoffee.com/connectankush)**

---

## ğŸ“„ License

MIT â€” do whatever you want with it. Break things responsibly. ğŸ’¥
